{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4ef60a-df15-4c7f-b9f7-ab4bde1de1e3",
   "metadata": {},
   "source": [
    "TASK 2: Robotics – Warehouse Robot Navigation\n",
    "Autonomous robots navigate warehouse aisles, detect obstacles using sensors, and optimize paths using reinforcement learning. Intelligence runs on edge devices, with cloud monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61aafa-6bb9-4ad5-ab26-5dc38da75b16",
   "metadata": {},
   "source": [
    "Problem statement-Modern warehouses increasingly rely on autonomous robots to transport goods, manage inventory, and improve operational efficiency. These robots must safely navigate narrow aisles, avoid static and dynamic obstacles, and reach target locations without collisions. Traditional rule-based navigation systems are insufficient in complex and dynamic warehouse environments.\n",
    "\n",
    "This project focuses on designing an intelligent warehouse robot navigation system using sensor data (LIDAR and ultrasonic sensors) combined with machine learning and reinforcement learning techniques. A neural network–based obstacle detection model is used to predict potential collisions, while a reinforcement learning approach optimizes the robot’s navigation path by maximizing safety and efficiency.\n",
    "\n",
    "The solution follows an edge-cloud architecture, where the model is trained in AWS SageMaker and deployed on robot edge devices using AWS Greengrass. Real-time inference is performed locally on the robot to minimize latency, while critical events such as collision alerts are sent to the cloud for monitoring.\n",
    "\n",
    "The objective of this project is to build a scalable, low-latency, and intelligent robotic navigation system suitable for real-world warehouse automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e79c0c33-7f8b-4227-95dd-2407b4bcb7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell connects SageMaker to S3 and prepares the dataset for training.\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "s3_input_path = \"s3://warehouse-robot-dataset/path/to/data/\"\n",
    "\n",
    "training_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_input_path,\n",
    "    content_type=\"csv\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "649fca9a-b2ea-492c-a2d8-052aeb85cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "#This code downloads all files from a specified S3 bucket folder to a local SageMaker directory for local processing.\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket = \"warehouse-robot-dataset\"\n",
    "prefix = \"path/to/data/\"\n",
    "local_dir = \"/home/ec2-user/SageMaker/data\"\n",
    "\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    file_name = os.path.basename(obj[\"Key\"])\n",
    "    if file_name:\n",
    "        s3.download_file(\n",
    "            bucket,\n",
    "            obj[\"Key\"],\n",
    "            os.path.join(local_dir, file_name)\n",
    "        )\n",
    "\n",
    "print(\"Download complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7533d4a-dc9f-423b-bb39-62669b6e21c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to: s3://warehouse-robot-dataset/data/autonomous_navigation_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "#This code uploads the local dataset from the SageMaker directory to an S3 bucket and returns the S3 URI so it can be used for training or processing jobs.\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "local_path = \"/home/ec2-user/SageMaker/data\"\n",
    "s3_uri = session.upload_data(\n",
    "    path=local_path,\n",
    "    bucket=\"warehouse-robot-dataset\",\n",
    "    key_prefix=\"data/autonomous_navigation_dataset.csv\"\n",
    ")\n",
    "\n",
    "print(\"Uploaded to:\", s3_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ac3982e-92e4-4d3f-961f-07fa89ccfb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autonomous_navigation_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "#This code lists and prints all object (file) names stored in the specified S3 bucket.\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=\"warehouse-robot-dataset\",\n",
    "    Prefix=\"\"\n",
    ")\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15d23a26-a4c6-4ae9-af34-95aa635cdbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lidar_min</th>\n",
       "      <th>lidar_max</th>\n",
       "      <th>ultrasonic_left</th>\n",
       "      <th>ultrasonic_right</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>orientation</th>\n",
       "      <th>battery_level</th>\n",
       "      <th>collision_flag</th>\n",
       "      <th>path_smoothness</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>316.54</td>\n",
       "      <td>98.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34</td>\n",
       "      <td>timeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>253.94</td>\n",
       "      <td>98.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34</td>\n",
       "      <td>timeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.97</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>108.54</td>\n",
       "      <td>97.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34</td>\n",
       "      <td>timeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.24</td>\n",
       "      <td>314.48</td>\n",
       "      <td>97.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34</td>\n",
       "      <td>timeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.34</td>\n",
       "      <td>4.96</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.41</td>\n",
       "      <td>233.75</td>\n",
       "      <td>96.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34</td>\n",
       "      <td>timeout</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_id  timestamp  lidar_min  lidar_max  ultrasonic_left  ultrasonic_right  \\\n",
       "0       1          0       0.76       3.04             0.97              1.63   \n",
       "1       1          1       2.16       4.84             0.95              0.82   \n",
       "2       1          2       1.48       4.97             1.11              1.37   \n",
       "3       1          3       0.66       4.32             1.24              0.86   \n",
       "4       1          4       0.34       4.96             1.46              1.86   \n",
       "\n",
       "      x     y  orientation  battery_level  collision_flag  path_smoothness  \\\n",
       "0  0.13  0.10       316.54          98.90               0             0.83   \n",
       "1  0.14  0.43       253.94          98.32               0             0.83   \n",
       "2  0.17  0.88       108.54          97.96               0             0.83   \n",
       "3  0.46  1.24       314.48          97.26               0             0.83   \n",
       "4  0.59  1.41       233.75          96.87               0             0.83   \n",
       "\n",
       "   time_taken   target  \n",
       "0          34  timeout  \n",
       "1          34  timeout  \n",
       "2          34  timeout  \n",
       "3          34  timeout  \n",
       "4          34  timeout  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"s3://warehouse-robot-dataset/autonomous_navigation_dataset.csv\"\n",
    ")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "324bffe1-fe18-4901-b91d-f0d5e1091acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3135, 14)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccb97e8-62e3-4a51-b156-2bc764ed7717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "#Obstacle Detection Model (Sensor-Based)\n",
    "#This class creates a custom PyTorch dataset that converts warehouse sensor data into input features and collision labels for training a machine learning model.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WarehouseSensorDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.features = df[\n",
    "            [\"lidar_min\", \"lidar_max\",\n",
    "             \"ultrasonic_left\", \"ultrasonic_right\",\n",
    "             \"x\", \"y\", \"orientation\", \"battery_level\"]\n",
    "        ].values\n",
    "\n",
    "        self.labels = df[\"collision_flag\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "326a5c8f-34c5-418c-9254-b4d98d64ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅ Obstacle Detection Model (MLP)\n",
    "#This neural network model takes sensor inputs and predicts whether an obstacle or collision is present using fully connected layers.\n",
    "import torch.nn as nn\n",
    "\n",
    "class ObstacleDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125e65a0-8fcb-4a91-946c-c863f6247c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0322\n",
      "Epoch 2 Loss: 0.0179\n",
      "Epoch 3 Loss: 0.0317\n",
      "Epoch 4 Loss: 0.0085\n",
      "Epoch 5 Loss: 0.0086\n"
     ]
    }
   ],
   "source": [
    "#Train Obstacle Detector\n",
    "#This code trains the obstacle detection model by loading sensor data in batches, calculating prediction loss, and updating the model weights using backpropagation.\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "dataset = WarehouseSensorDataset(df)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = ObstacleDetector()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed4e63f-56c7-4d57-859d-37dbbd667046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: Reinforcement Learning for Path Optimization (DQN)\n",
    "#This class defines a custom environment for reinforcement learning. It provides the robot's current state (sensor readings), computes a reward based on path smoothness, collisions, and time, and moves to the next step in the dataset.\n",
    "class WarehouseEnv:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = 0\n",
    "        self.state_cols = [\n",
    "            \"lidar_min\", \"lidar_max\",\n",
    "            \"ultrasonic_left\", \"ultrasonic_right\",\n",
    "            \"x\", \"y\", \"orientation\", \"battery_level\"\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "        self.idx = 0\n",
    "        return self._state()\n",
    "\n",
    "    def _state(self):\n",
    "        return self.df.loc[self.idx, self.state_cols].values.astype(\"float32\")\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.loc[self.idx]\n",
    "\n",
    "        reward = (\n",
    "            row[\"path_smoothness\"] * 2\n",
    "            - row[\"collision_flag\"] * 10\n",
    "            - row[\"time_taken\"] * 0.01\n",
    "        )\n",
    "\n",
    "        self.idx += 1\n",
    "        done = self.idx >= len(self.df) - 1\n",
    "        return self._state(), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162d48dd-c641-451c-b4b6-6ed13cb0f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN Model\n",
    "#This is a Deep Q-Network (DQN) model for reinforcement learning. It takes the robot’s sensor state as input and outputs Q-values for four possible actions (forward, left, right, stop) to decide the best move.\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a0904c-356b-4cbd-a367-7b08d9b324b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseEnv(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b2fca3-0b7f-4b54-ae64-d5389f897596",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe23263-92cd-46bb-8581-b61fde7c30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected action: 0\n"
     ]
    }
   ],
   "source": [
    "#This code runs the trained DQN model in evaluation mode on the current state without updating weights. It computes Q-values for all possible actions and selects the action with the highest value.\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action = model(state_tensor).argmax(dim=1).item()\n",
    "\n",
    "print(\"Selected action:\", action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae40e15-db81-4ee8-956a-7d55281c4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1.3199999999999998\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "#This code executes the selected action in the environment, returning the next state, the reward received for that action, and whether the episode has finished.\n",
    "next_state, reward, done = env.step(action)\n",
    "\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ddbec09-7c31-4b25-83a5-f0dc9beec08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward: 1941.9899999999761\n"
     ]
    }
   ],
   "source": [
    "#state = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = model(state_tensor).argmax(dim=1).item()\n",
    "\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total Episode Reward:\", total_reward)\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = model(state_tensor).argmax(dim=1).item()\n",
    "\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total Episode Reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c87d4a-cf18-4d3a-a589-31e64e842d98",
   "metadata": {},
   "source": [
    "Warehouse Robot Navigation using AI & IoT\n",
    "\n",
    "This project focuses on autonomous warehouse robot navigation using sensor data and reinforcement learning. Robots are equipped with LIDAR, ultrasonic sensors, and cameras to perceive the environment and detect obstacles in real time. Sensor data is processed by an obstacle detection model to identify potential collisions.\n",
    "\n",
    "A reinforcement learning (DQN) agent optimizes navigation decisions such as moving forward, turning, or stopping. The agent receives rewards based on path smoothness, collision avoidance, and time efficiency. This ensures safe and optimal navigation within warehouse aisles.\n",
    "\n",
    "Inference is deployed on edge devices for low latency decision-making. Cloud platforms such as AWS Greengrass or Azure IoT Hub are used for monitoring, alerting, and performance logging. Experimental results show a high cumulative reward, indicating efficient and collision-free navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496c6d3e-715e-4010-8f6a-07404bb7a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves trained RL model\n",
    "\n",
    "#This file will be deployed to edge device\n",
    "torch.save(model.state_dict(), \"warehouse_robot_dqn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f8cc43e-67ca-4506-aa3f-f89db608f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Model uploaded to: s3://warehouse-robot-dataset/greengrass/models/warehouse_robot_dqn.pt\n"
     ]
    }
   ],
   "source": [
    "#Upload Model to S3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "s3_model_path = session.upload_data(\n",
    "    path=\"warehouse_robot_dqn.pt\",\n",
    "    bucket=\"warehouse-robot-dataset\",\n",
    "    key_prefix=\"greengrass/models\"\n",
    ")\n",
    "\n",
    "print(\"Model uploaded to:\", s3_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386e0146-aa7f-4b12-a600-e4fe33c350c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Model on Edge Device (Greengrass Core)\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "s3.download_file(\n",
    "    \"warehouse-robot-dataset\",\n",
    "    \"greengrass/models/warehouse_robot_dqn.pt\",\n",
    "    \"warehouse_robot_dqn.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76cdc166-fcd0-4d61-991b-360a08c73e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4)   # 4 actions\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ca67be-2830-4c15-89ae-152d0d3105e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObstacleDetector(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ObstacleDetector()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"warehouse_robot_dqn.pt\", map_location=\"cpu\")\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b38f7ce-4aba-4f7a-97b6-0e5aa3879dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_collision(state):\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        pred = model(x).argmax(dim=1).item()\n",
    "    return pred  # 1 = collision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97bc2605-3cdc-4e4a-995a-3ab534d2c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if infer_collision(state) == 1:\n",
    "    send_alert({\"alert\": \"Collision detected\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c1230-f086-4a45-a94b-1c804c5fc2cd",
   "metadata": {},
   "source": [
    "Summary-This project implements an autonomous warehouse robot navigation system using sensor-driven machine learning and reinforcement learning techniques. LIDAR and ultrasonic sensor data were processed to construct a state representation for obstacle detection and navigation decision-making. A supervised neural network model was trained in AWS SageMaker to classify collision risk, while a reinforcement learning–based policy optimized navigation actions by maximizing a reward function that penalizes collisions and inefficient paths.\n",
    "\n",
    "The trained models were prepared for deployment using an edge-cloud architecture, where AWS Greengrass enables low-latency inference directly on robotic edge devices. This approach minimizes cloud dependency during real-time operation while allowing cloud-based monitoring and alerting for collision and path anomalies. The system supports scalable deployment, real-time decision-making, and reliable monitoring, making it suitable for intelligent warehouse automation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067a020-b7e9-4dfc-8efd-46dc59379239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
